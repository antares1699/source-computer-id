{"title":"Mendownload semua Url yang dapat diakses di bawah domain tertentu dengan wget tanpa menyimpan halaman sebenarnya?","draft":false,"url":"/mendownload-semua-url-yang-dapat-diakses-di-bawah-domain-tertentu-dengan-wget-tanpa-menyimpan-halaman-sebenarnya-mgrcslob","votes":"4","answers_count":"2","categories":["Komputer"],"tags":["linux","bash","python","wget"],"snippet":"Hai, mencoba menentukan semua url yang valid di bawah domain tertentu tanpa harus menduplikasi situs secara lokal.  Orang-orang pada umumnya ingin mengunduh semua halaman tetapi saya hanya ingin mendapatkan daftar url langsung...","body":"<p> Hai, mencoba menentukan semua url yang valid di bawah domain tertentu tanpa harus menduplikasi situs secara lokal. </p>\n\n<p> Orang-orang pada umumnya ingin mengunduh semua halaman tetapi saya hanya ingin mendapatkan daftar url langsung di bawah domain tertentu (misalnya www.example.com), yang akan menjadi seperti www.example.com/page1, www.example. com / page2, dll. </p>\n\n<p> Apakah ada cara menggunakan wget untuk melakukan ini? atau apakah ada alat yang lebih baik untuk ini? </p>","source":"https://superuser.com/questions/649778/dowloading-all-urls-accessible-under-a-given-domain-with-wget-without-saving-the","author":"<a href=\"https://superuser.com/users/100684/fccoelho\" target=\"_blank\" rel=\"nofollow noopener\"><span itemprop=\"name\">fccoelho</span></a>","replies":[{"reply":"Untuk menentukan link pada setiap halaman, Anda perlu melihat halaman tersebut (Yaitu Download)","author":"<a href=\"https://superuser.com/users/144802/brian-adkins\" target=\"_blank\" rel=\"nofollow noopener\">Brian Adkins</a>"},{"reply":"@ BrianAdkins: Saya baik-baik saja dengan mengunduh tetapi saya hanya ingin menyimpan url bukan konten halaman","author":"<a href=\"https://superuser.com/users/100684/fccoelho\" target=\"_blank\" rel=\"nofollow noopener\">fccoelho</a>"},{"reply":"Ada opsi <code>--spider</code> yang mengunduh halaman, tetapi tidak menyimpannya.","author":"<a href=\"https://superuser.com/users/51705/lawrencec\" target=\"_blank\" rel=\"nofollow noopener\">LawrenceC</a>"}],"answers":[{"answer":"<p> Berikut ini skrip kasarnya: </p> <pre><code>curl -s whaturl |\n  grep -o \"&lt;a href=[^&gt;]*&gt;\" |\n  sed -r 's/&lt;a href=\"([^\"]*)\".*&gt;/\\1/' |\n  sort -u\n</code></pre> <p> <code>grep</code> mengambil semua <code>href</code>. <code>sed</code> memilih bagian url dari <code>href</code>. <code>sort</code> memfilter tautan duplikat. </p>\n\n<p> Ini juga akan bekerja dengan <code>wget -O -</code> menggantikan <code>curl -s</code>. </p>\n\n<p> Contoh keluaran: </p> <pre><code>$ curl -s http://stackexchange.com/users/148837/lesmana?tab=accounts | grep -o \"&lt;a href=[^&gt;]*&gt;\" | sed -r 's/&lt;a href=\"([^\"]*)\".*&gt;/\\1/' | sort -u\n/\n/about\n/about/contact\n/blogs\n/leagues\n/legal\n/legal/privacy-policy\n/newsletters\n/questions\n/sites\n/users/148837/lesmana\n/users/148837/lesmana?tab=activity\n/users/148837/lesmana?tab=favorites\n/users/148837/lesmana?tab=reputation\n/users/148837/lesmana?tab=subscriptions\n/users/148837/lesmana?tab=top\n/users/login?returnurl=%2fusers%2f148837%2flesmana%3ftab%3daccounts\nhttp://area51.stackexchange.com/users/16563/lesmana\nhttp://askubuntu.com/users/1366/\nhttp://blog.stackexchange.com\nhttp://blog.stackoverflow.com/2009/06/attribution-required/\nhttp://chat.stackexchange.com/\nhttp://creativecommons.org/licenses/by-sa/3.0/\nhttp://gaming.stackexchange.com/users/2790/\nhttp://meta.stackoverflow.com\nhttp://meta.stackoverflow.com/users/147747/\nhttp://programmers.stackexchange.com/users/116/\nhttp://serverfault.com/users/45166/\nhttp://stackoverflow.com/users/360899/\nhttp://superuser.com/users/39401/\nhttp://twitter.com/stackexchange\nhttp://unix.stackexchange.com/users/1170/\nhttp://www.facebook.com/stackexchange\nhttps://plus.google.com/+StackExchange\n</code></pre>","votes":"2","source":"https://superuser.com/a/649794","author":"<a href=\"https://superuser.com/users/39401/lesmana\" target=\"_blank\" rel=\"nofollow noopener\"><span itemprop=\"name\">lesmana</span></a>","replies":[{"reply":"Sangat bagus! Saya telah mengabaikan CURL karena tidak dapat muncul kembali. Saya menemukan itu <b> httrack </b> memecahkan masalah ini dengan memadai.","author":"<a href=\"https://superuser.com/users/100684/fccoelho\" target=\"_blank\" rel=\"nofollow noopener\">fccoelho</a>"}]},{"answer":"<p> Oke, saya harus menemukan jawaban saya sendiri: </p>\n\n<p> alat yang saya gunakan adalah <strong> httrack </strong> . </p> <pre><code>httrack -p0 -r2 -d www.example.com\n</code></pre> <ul>\n<li> itu <em> -p0 </em> opsi memberitahukannya untuk hanya memindai (bukan menyimpan halaman); </li>\n<li> itu <em> -rx </em> Opsi memberitahukan kedalaman pencarian </li>\n<li> itu <em> -d </em> options memberitahu itu untuk tetap di domain utama yang sama </li>\n</ul>\n\n<p> bahkan ada -% L untuk menambahkan URL yang dipindai ke file yang ditentukan tetapi tampaknya tidak berhasil. Tetapi itu tidak menjadi masalah, karena di bawah direktori hts-cache Anda dapat menemukan file <strong> TSV </strong> file bernama <em> new.txt </em> berisi semua url yang dikunjungi dan beberapa informasi tambahan tentangnya. Saya bisa mengekstrak URL darinya dengan kode python berikut: </p> <pre><code>with open(\"hts-cache/new.txt\") as f:\n    t = csv.DictReader(f,delimiter='\\t')\n    for l in t:\n        print l['URL']\n</code></pre>","votes":"4","source":"https://superuser.com/a/649810","author":"<a href=\"https://superuser.com/users/100684/fccoelho\" target=\"_blank\" rel=\"nofollow noopener\"><span itemprop=\"name\">fccoelho</span></a>","replies":[]}]}
